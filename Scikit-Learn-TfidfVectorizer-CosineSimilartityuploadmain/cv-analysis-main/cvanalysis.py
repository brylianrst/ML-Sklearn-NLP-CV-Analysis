# -*- coding: utf-8 -*-
"""CVAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H7dakzC94gZuBjXVMygjEgn87_RiXF_Q
"""

# Install necessary packages
!pip install PyMuPDF
!pip install prettytable
!pip install python-docx

from multiprocessing import Pool, cpu_count
import pandas as pd
import nltk
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import os
import fitz  # PyMuPDF for extracting text from PDFs
from docx import Document
import re
from google.colab import drive
from prettytable import PrettyTable
import time

# Mount Google Drive
drive.mount('/content/drive')

# Path to the folder containing CVs in Google Drive
cv_folder_path = '/content/drive/My Drive/FBACVs'

# List all PDF, DOC, and DOCX files in the CV folder
cv_files = [f for f in os.listdir(cv_folder_path) if f.lower().endswith(('.pdf', '.doc', '.docx'))]

# Sample Job Description
sample_jd = """
Job Description: Marketing Specialist at XYZ Company
Location: London, UK
Department: Marketing
Position Type: Full-Time

About XYZ Company:

XYZ Company is a leading innovator in the technology sector, committed to delivering high-quality products and services that empower businesses worldwide. With a dynamic and collaborative work environment, we pride ourselves on fostering creativity, growth, and excellence in our team. As we continue to expand our market presence, we are looking for a talented and driven Marketing Specialist to join our team in London, UK.

Position Overview:

We are seeking a creative and analytical Marketing Specialist to help us elevate our brand and drive our marketing initiatives to new heights. The ideal candidate will be responsible for developing, implementing, and managing marketing campaigns that promote our products and services. This role will involve a mix of strategic planning, creative thinking, and hands-on execution. If you are passionate about marketing, thrive in a fast-paced environment, and have a keen eye for detail, we want to hear from you!

Key Responsibilities:

Campaign Management: Develop and execute comprehensive marketing campaigns across various channels, including digital, social media, email, and traditional marketing.
Content Creation: Create compelling and engaging content for different platforms, including blog posts, social media updates, newsletters, and website copy.
Market Research: Conduct market research to identify trends, customer needs, and competitive analysis to inform marketing strategies.
Brand Development: Work closely with the design and product teams to ensure consistent branding and messaging across all marketing materials.
Performance Analysis: Track and analyze the performance of marketing campaigns using tools such as Google Analytics, and provide actionable insights and recommendations.
SEO/SEM: Optimize content for search engines and manage paid search campaigns to increase online visibility and drive traffic.
Social Media Management: Manage and grow our social media presence by creating and curating high-quality content and engaging with our audience.
Event Coordination: Plan and execute marketing events, webinars, and trade shows to promote our brand and generate leads.
Collaborative Projects: Collaborate with cross-functional teams, including sales, product development, and customer service, to align marketing efforts with overall business goals.
Qualifications:

Education: Bachelor’s degree in Marketing, Business, Communications, or a related field.
Experience: 3+ years of experience in marketing, preferably within the technology or B2B sector.
Skills: Strong understanding of digital marketing channels, SEO/SEM, content marketing, and social media strategies.
Tools: Proficient in marketing tools and platforms such as Google Analytics, Hootsuite, MailChimp, and Adobe Creative Suite.
Creativity: Exceptional creativity and innovation skills, with the ability to develop unique marketing strategies and content.
Analytical Thinking: Strong analytical skills with the ability to interpret data and make data-driven decisions.
Communication: Excellent written and verbal communication skills, with a keen attention to detail.
Team Player: Ability to work effectively both independently and as part of a team in a fast-paced environment.
Adaptability: Highly adaptable with a positive attitude and a willingness to learn and take on new challenges.
What We Offer:

Competitive salary and performance-based bonuses.
Comprehensive benefits package including health insurance, retirement plans, and paid time off.
Opportunities for professional development and career growth.
A vibrant and inclusive work environment with a focus on work-life balance.
The chance to be part of a forward-thinking company at the forefront of technological innovation.
How to Apply:

If you are excited about the opportunity to contribute to a growing company and make a significant impact, please submit your resume and a cover letter detailing your relevant experience and why you are the perfect fit for this role. Apply now and join us in shaping the future of technology at XYZ Company!

XYZ Company is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees."""

# Record the start time
start_time = time.time()

# Function to extract name, designation, experience, education, and skills
def extract_information(text):
    name = ''
    designation = ''
    experience = ''
    education = ''
    skills = ''

    # Define regular expressions to match patterns
    name_pattern = r'Name: ([A-Za-z\s]+)'
    designation_pattern = r'Designation: ([\w\s]+)'
    experience_pattern = r'Experience:\s*(.*?)\s*(?=Education:|Skills:|$)'  # Updated experience pattern to capture different formats
    education_pattern = r'Education: (.*?)(?=[A-Z][a-z]+:|$)'
    skills_pattern = r'Skills: (.*?)(?=[A-Z][a-z]+:|$)'

    name_match = re.search(name_pattern, text)
    if name_match:
        name = name_match.group(1).strip()

    designation_match = re.search(designation_pattern, text)
    if designation_match:
        designation = designation_match.group(1).strip()

    experience_match = re.search(experience_pattern, text, re.DOTALL)
    if experience_match:
        experience = experience_match.group(1).strip()

    education_match = re.search(education_pattern, text, re.DOTALL)
    if education_match:
        education = education_match.group(1).strip()

    skills_match = re.search(skills_pattern, text, re.DOTALL)
    if skills_match:
        skills = skills_match.group(1).strip()

    # Additional cleanup: Remove bullet points and other unwanted characters
    unwanted_chars = ["•", "●", "▪", "§", "\n", "\r", "○"]
    for char in unwanted_chars:
        name = name.replace(char, "").strip()
        designation = designation.replace(char, "").strip()
        experience = experience.replace(char, "").strip()
        education = education.replace(char, "").strip()
        skills = skills.replace(char, "").strip()

    # Remove 'Email' from the name field if present
    if 'email' in name.lower():
        name = re.split(r'\s+', name, 1)[0]

    return name, designation, experience, education, skills

# Function to read the content of DOCX file
def read_docx(file_path):
    doc = Document(file_path)
    full_text = [para.text for para in doc.paragraphs]
    return '\n'.join(full_text)

# Function to read the content of DOC file (using antiword tool)
def read_doc(file_path):
    result = os.popen(f'antiword "{file_path}"').read()
    return result

# Function to process each file and extract information
def process_cv_file(cv_file):
    file_path = os.path.join(cv_folder_path, cv_file)
    text = ""
    if cv_file.lower().endswith('.pdf'):
        with fitz.open(file_path) as pdf:
            for page in pdf:
                text += page.get_text()
    elif cv_file.lower().endswith('.docx'):
        text = read_docx(file_path)
    elif cv_file.lower().endswith('.doc'):
        text = read_doc(file_path)

    name, designation, experience, education, skills = extract_information(text)
    return cv_file, text, name, designation, experience, education, skills

# Parallelize the processing of CV files
with Pool(cpu_count()) as p:
    results = p.map(process_cv_file, cv_files)

# Unzip the results
file_names, cv_texts, names, designations, experiences, educations, skills_list = zip(*results)

# Create a DataFrame from the CV texts and extracted information
cvs_df = pd.DataFrame({
    "File Name": file_names,
    "Text": cv_texts,
    "Name": names,
    "Designation": designations,
    "Experience": experiences,
    "Education": educations,
    "Skills": skills_list
})

# Download NLTK resources if not already downloaded
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize WordNet Lemmatizer
lemmatizer = WordNetLemmatizer()

# Function to get synonyms of a word using WordNet
def get_synonyms(word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for lemma in syn.lemmas():
            synonyms.add(lemma.name())
    return synonyms

# Function to preprocess text for TF-IDF vectorization
def preprocess_text(text, jd_keywords_synonyms=None):
    words = nltk.word_tokenize(text.lower())
    stop_words = set(nltk.corpus.stopwords.words('english'))
    words = [lemmatizer.lemmatize(word) for word in words if word.isalnum() and word not in stop_words]

    if jd_keywords_synonyms:
        words_with_synonyms = set(words)
        for word in words:
            if word in jd_keywords_synonyms:
                words_with_synonyms.update(jd_keywords_synonyms[word])
        words = list(words_with_synonyms)

    return ' '.join(words)

# Extract keywords from the JD and find their synonyms
jd_keywords = nltk.word_tokenize(sample_jd.lower())
jd_keywords = [lemmatizer.lemmatize(word) for word in jd_keywords if word.isalnum() and word not in set(nltk.corpus.stopwords.words('english'))]

jd_keywords_synonyms = {}
for keyword in jd_keywords:
    jd_keywords_synonyms[keyword] = get_synonyms(keyword)

# Function to calculate context score based on cosine similarity of TF-IDF vectors
def calculate_context_score(jd, cv):
    jd_processed = preprocess_text(jd)
    cv_processed = preprocess_text(cv, jd_keywords_synonyms)
    documents = [jd_processed, cv_processed]
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(documents)
    cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])
    return cosine_sim[0][0]

# Function to calculate scores for each CV
def calculate_scores(i):
    combined_text = f"{cvs_df.iloc[i]['Text']} {cvs_df.iloc[i]['Name']} {cvs_df.iloc[i]['Experience']} {cvs_df.iloc[i]['Skills']}"
    context_score = calculate_context_score(sample_jd, combined_text)
    return context_score

# Parallelize score calculation using multiprocessing
with Pool(cpu_count()) as p:
    context_scores = p.map(calculate_scores, range(len(cvs_df)))

# Add context scores to the DataFrame
cvs_df['Context Score'] = context_scores

# Normalize scores (context score only)
cvs_df['Normalized Context Score'] = cvs_df['Context Score'] / cvs_df['Context Score'].max()

# Convert scores to percentages
cvs_df['Context Score (%)'] = cvs_df['Normalized Context Score'] * 100

# Define weight for context score (since Monte Carlo score is removed)
context_weight = 1.0

# Calculate composite score (only context score)
cvs_df['Composite Score (%)'] = cvs_df['Context Score (%)']

# Sort the DataFrame by composite score in descending order
sorted_cvs_df = cvs_df.sort_values(by='Composite Score (%)', ascending=False)

# Output the top-ranked CVs with their file names and scores (excluding Designation and Education)
top_cvs = sorted_cvs_df[['File Name', 'Composite Score (%)']]

# Print top CVs
print("Top CVs based on Composite Score:")
top_cvs_table = PrettyTable()
top_cvs_table.field_names = ["File Name", "Composite Score (%)"]

for index, row in top_cvs.head(10).iterrows():
    top_cvs_table.add_row([row["File Name"], row["Composite Score (%)"]])

print(top_cvs_table)

# Record the end time
end_time = time.time()

# Calculate and print the total execution time
execution_time = end_time - start_time
print(f"Total execution time: {execution_time:.2f} seconds")