# -*- coding: utf-8 -*-
"""CVAnalysis Benchmark Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XsHCqt3vRQLGwu3p1DimQxJG-g4S-fNG
"""

# Install necessary packages
!pip install PyMuPDF
!pip install prettytable
!pip install python-docx

import re
import os
import fitz
import time
import nltk
import logging
import numpy as np
import pandas as pd
from docx import Document
nltk.download('stopwords')
from google.colab import drive
from nltk.corpus import wordnet
from nltk.corpus import stopwords
from prettytable import PrettyTable
from nltk.stem import WordNetLemmatizer
from multiprocessing import Pool, cpu_count
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

# Setup logging
logging.basicConfig(level=logging.INFO)

# Mount Google Drive
drive.mount('/content/drive')

# Path to the folder containing CVs and job descriptions in Google Drive
cv_folder_path = '/content/drive/My Drive/FBACVs/dataset'
jd_folder_path = '/content/drive/My Drive/FBACVs/jd'

# Download NLTK resources if not already downloaded
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

# Function to extract information from text
def extract_information(text):
    name = ''
    designation = ''
    experience = ''
    education = ''
    skills = ''

    # Define regular expressions to match patterns
    name_pattern = r'Name: ([A-Za-z\s]+)'
    designation_pattern = r'Designation: ([\w\s]+)'
    experience_pattern = r'Experience:\s*(.*?)\s*(?=Education:|Skills:|$)'
    education_pattern = r'Education: (.*?)(?=[A-Z][a-z]+:|$)'
    skills_pattern = r'Skills: (.*?)(?=[A-Z][a-z]+:|$)'

    name_match = re.search(name_pattern, text)
    if name_match:
        name = name_match.group(1).strip()

    designation_match = re.search(designation_pattern, text)
    if designation_match:
        designation = designation_match.group(1).strip()

    experience_match = re.search(experience_pattern, text, re.DOTALL)
    if experience_match:
        experience = experience_match.group(1).strip()

    education_match = re.search(education_pattern, text, re.DOTALL)
    if education_match:
        education = education_match.group(1).strip()

    skills_match = re.search(skills_pattern, text, re.DOTALL)
    if skills_match:
        skills = skills_match.group(1).strip()

    # Additional cleanup
    unwanted_chars = ["•", "●", "▪", "§", "\n", "\r", "○"]
    for char in unwanted_chars:
        name = name.replace(char, "").strip()
        designation = designation.replace(char, "").strip()
        experience = experience.replace(char, "").strip()
        education = education.replace(char, "").strip()
        skills = skills.replace(char, "").strip()

    if 'email' in name.lower():
        name = re.split(r'\s+', name, 1)[0]

    return name, designation, experience, education, skills

# Function to read the content of DOCX file
def read_docx(file_path):
    doc = Document(file_path)
    full_text = [para.text for para in doc.paragraphs]
    return '\n'.join(full_text)

# Function to read the content of DOC file
def read_doc(file_path):
    result = os.popen(f'antiword "{file_path}"').read()
    return result

# Function to read the content of PDF file
def read_pdf(file_path):
    text = ""
    with fitz.open(file_path) as pdf:
        for page in pdf:
            text += page.get_text()
    return text

# Function to process each file and extract information
def process_cv_file(file_path):
    text = ""
    if file_path.lower().endswith('.pdf'):
        text = read_pdf(file_path)
    elif file_path.lower().endswith('.docx'):
        text = read_docx(file_path)
    elif file_path.lower().endswith('.doc'):
        text = read_doc(file_path)

    name, designation, experience, education, skills = extract_information(text)
    return os.path.basename(file_path), text, name, designation, experience, education, skills

# Function to process CVs in chunks of 100
def process_cvs_in_chunks(cv_files, chunk_size=100):
    results = []
    for i in range(0, len(cv_files), chunk_size):
        chunk_files = cv_files[i:i + chunk_size]
        file_paths = [os.path.join(cv_folder_path, f) for f in chunk_files]
        with Pool(cpu_count()) as p:
            chunk_results = p.map(process_cv_file, file_paths)
        results.extend(chunk_results)
    return results

# List all PDF, DOC, and DOCX files in the CV folder
cv_files = [f for f in os.listdir(cv_folder_path) if f.lower().endswith(('.pdf', '.doc', '.docx'))]

# Process CV files in chunks
start_time = time.time()
cv_results = process_cvs_in_chunks(cv_files)
end_time = time.time()
logging.info(f"Processed {len(cv_results)} CVs in {end_time - start_time:.2f} seconds")

# Unzip the results
file_names, cv_texts, names, designations, experiences, educations, skills_list = zip(*cv_results)

# Create a DataFrame from the CV texts and extracted information
cvs_df = pd.DataFrame({
    "File Name": file_names,
    "Text": cv_texts,
    "Name": names,
    "Designation": designations,
    "Experience": experiences,
    "Education": educations,
    "Skills": skills_list
})

# Lemmatize function
def lemmatize(text):
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))
    words = nltk.word_tokenize(text)
    filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]
    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]
    return ' '.join(lemmatized_words)

# Calculate the similarity between job description and CVs
def calculate_similarity(jd, cv):
    jd_lem = lemmatize(jd)
    cv_lem = lemmatize(cv)
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform([jd_lem, cv_lem])
    return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]

# Function to read job description files
def read_job_description(file_path):
    text = ""
    if file_path.lower().endswith('.pdf'):
        text = read_pdf(file_path)
    elif file_path.lower().endswith('.docx'):
        text = read_docx(file_path)
    elif file_path.lower().endswith('.doc'):
        text = read_doc(file_path)
    return text

# Specify the job description file
job_description_file = 'Python Developer JD.docx'  # Update this to your specific job description file
jd_path = os.path.join(jd_folder_path, job_description_file)

# Read the job description
jd_text = read_job_description(jd_path)
sample_jd = jd_text  # Use the content of the job description file

# Evaluate each CV against the specified job description
cvs_df['Similarity'] = cvs_df['Text'].apply(lambda x: calculate_similarity(sample_jd, x))

# Sort the DataFrame by 'Similarity' in descending order and get the top 10
top_10_cvs = cvs_df.sort_values(by='Similarity', ascending=False).head(10)

# Display the top 10 results in a table format
results_table = PrettyTable()
results_table.field_names = ["File Name", "Similarity (%)"]

for index, row in top_10_cvs.iterrows():
    similarity_percentage = row['Similarity'] * 100
    results_table.add_row([row['File Name'], f"{similarity_percentage:.2f}%"])

print(f"Top 10 CVs for Job Description: {job_description_file}")
print(results_table)